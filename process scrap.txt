Le Playbook "Job Alert" : Processus de Scraping Garanti (Version Finale)
Ce processus en 3 phases est notre strat√©gie unique et standardis√©e pour int√©grer une nouvelle banque au projet. Il privil√©gie la fiabilit√© et la pr√©visibilit√© en √©liminant les devinettes.

Phase 1 : Mission d'Extraction HTML (Le "Relev√© Topographique")
üéØ Objectif : Obtenir une "photographie" parfaite et compl√®te du code HTML de la page des offres, exactement comme un navigateur la voit apr√®s que tout le JavaScript a fini de s'ex√©cuter. C'est la base de tout notre travail.

üë®‚Äçüíª Le Script Outil : Pour chaque nouvelle banque, nous utilisons ce script "d'extraction" comme point de d√©part.

Generated python
# Fichier : fetchers/NOM_BANQUE.py (Script de la Mission d'Extraction)

from playwright.sync_api import sync_playwright
from typing import List
from models import JobPosting # Importer le mod√®le m√™me s'il n'est pas utilis√© ici
import time

# ---- VARIABLES √Ä CONFIGURER ----
# Mettre l'URL exacte de la page o√π les offres sont list√©es
SEARCH_PAGE_URL = "URL_DE_LA_PAGE_DES_OFFRES" 
# Le nom du fichier de sortie sera bas√© sur cet identifiant
ID_BANQUE = "cle_courte_banque" 
# ------------------------------

USER_AGENT_STRING = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'

def fetch(keyword: str, hours: int, limit: int, **bank_args) -> List[JobPosting]:
    print(f"--- D√âMARRAGE DE LA MISSION D'EXTRACTION HTML POUR {ID_BANQUE.upper()} ---")
    
    with sync_playwright() as p:
        # On utilise headless=True pour que √ßa tourne en fond.
        # Si √ßa √©choue, on peut passer √† False pour voir ce qui se passe.
        browser = p.chromium.launch(headless=True) 
        context = browser.new_context(user_agent=USER_AGENT_STRING)
        page = context.new_page()
        page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        try:
            print(f"  1. Navigation vers {SEARCH_PAGE_URL}...")
            page.goto(SEARCH_PAGE_URL, wait_until='domcontentloaded', timeout=40000)

            print("  2. Attente de 15 secondes pour garantir le chargement complet...")
            time.sleep(15) 

            print("  3. R√©cup√©ration du contenu HTML de la page...")
            html_content = page.content()

            output_filename = f"debug_page_{ID_BANQUE}.html"
            with open(output_filename, "w", encoding="utf-8") as f:
                f.write(html_content)
            
            print(f"\n‚úÖ MISSION ACCOMPLIE ! Fichier '{output_filename}' cr√©√©.")

        except Exception as e:
            print(f"  ‚ùå Erreur durant la mission : {e}")
            import traceback
            traceback.print_exc()
        finally:
            browser.close()
    
    # On retourne toujours une liste vide durant cette phase
    return []
Use code with caution.
Python
‚úÖ Tes Instructions pour la Phase 1 :

Cr√©er le nouveau fichier fetchers/nom_banque.py.

Copier le code ci-dessus et configurer les variables SEARCH_PAGE_URL et ID_BANQUE.

Int√©grer temporairement ce fetcher dans collector.py et config.yaml.

Lancer python collector.py pour g√©n√©rer le fichier debug_page_nom_banque.html.

Phase 2 : Analyse Chirurgicale du HTML (La "Lecture de Plans")
üéØ Objectif : Agir comme un architecte. √Ä partir du fichier HTML brut, identifier pr√©cis√©ment la structure et les "mat√©riaux" (balises, classes) utilis√©s pour construire une seule carte d'offre.

üî¨ Tes Instructions pour la Phase 2 :

Ouvrir le fichier debug_page_nom_banque.html g√©n√©r√© dans ton √©diteur de code.

Utiliser la recherche (Ctrl+F) pour localiser le titre d'une offre d'emploi r√©elle.

√Ä partir de cette ligne, remonter dans le code pour identifier la balise parente qui encapsule la carte enti√®re (typiquement <article>, <div>, ou <li>).

Noter la balise et la classe de cet √©l√©ment parent (ex: div.job-list-item). C'est notre s√©lecteur principal.

Analyser le contenu de cette carte pour rep√©rer les s√©lecteurs des informations cl√©s :

Titre de l'offre.

Lien (href).

Lieu.

Type de contrat.

Nom de l'entit√©/compagnie.

Me fournir l'extrait de code HTML de cette carte (30-40 lignes) pour validation.

Phase 3 : D√©veloppement et Int√©gration du Fetcher Final
üéØ Objectif : Construire le script de scraping final, autonome et robuste, en utilisant les plans pr√©cis de la Phase 2, puis l'int√©grer compl√®tement au projet.

üë®‚Äçüíª Mon R√¥le (puis le tien pour l'int√©gration) :

√Ä partir de ton extrait HTML, je te fournis le code final et fonctionnel du fetcher, avec :

La gestion du clic sur le bandeau de cookies (si n√©cessaire).

L'attente explicite du s√©lecteur principal que nous avons identifi√©.

L'extraction de toutes les donn√©es avec les bons s√©lecteurs.

L'utilisation de nos classifieurs pour standardiser les donn√©es.

L'assignation de la source avec la ID_BANQUE d√©finie en Phase 1.

Tu remplaces le code de ton fetcher par cette version finale.

Tu proc√®des √† l'int√©gration compl√®te :

Backend : V√©rifier que collector.py et config.yaml sont bien configur√©s pour le nouveau fetcher.

Frontend : Ajouter la nouvelle banque √† ui/src/config/banks.ts.

Synchronisation : Supprimer les jobs.db existants, relancer collector.py, puis red√©marrer le serveur Next.js.